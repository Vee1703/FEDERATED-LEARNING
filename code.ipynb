{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*8*8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 64*8*8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create heterogenous data loaders\n",
    "def create_heterogenous_data_loaders(dataset, num_users):\n",
    "    heterogenous_loaders = []\n",
    "    num_samples_per_user = len(dataset) // num_users\n",
    "    for i in range(num_users):\n",
    "        user_indices = list(range(i * num_samples_per_user, (i + 1) * num_samples_per_user))\n",
    "        user_loader = DataLoader(Subset(dataset, user_indices), batch_size=64, shuffle=True)\n",
    "        heterogenous_loaders.append(user_loader)\n",
    "    return heterogenous_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedLearning:\n",
    "    def __init__(self, num_users, num_epochs, algorithm='FedAvg', device='cuda'):\n",
    "        self.num_users = num_users\n",
    "        self.num_epochs = num_epochs\n",
    "        self.algorithm = algorithm\n",
    "        self.local_criteria = [nn.CrossEntropyLoss() for _ in range(num_users)]\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.local_optimizers = [None for _ in range(num_users)]\n",
    "        self.optimizer = None\n",
    "        self.device = device\n",
    "\n",
    "    def train_local_model_avg(self, model, train_loader, optimizer, criterion):\n",
    "        model.train()\n",
    "        model.to(self.device)\n",
    "        for _ in range(2):\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def train_local_model_adam(self, model, train_loader, optimizer, criterion):\n",
    "        model.train()\n",
    "        model.to(self.device)\n",
    "        avg_grad = {}\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "        avg_grad = {name: param.grad for name, param in model.named_parameters()}\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()        \n",
    "        return avg_grad\n",
    "    \n",
    "    def train_local_model_prox(self, model, train_loader, optimizer, criterion, global_model, mu=0.01):\n",
    "        model.train()\n",
    "        model.to(self.device)\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss += mu * sum([torch.norm(param - global_param) for param, global_param in zip(model.parameters(), global_model.parameters())])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def fed_avg(self, global_model, local_models):\n",
    "        global_model.train()\n",
    "        global_model.to(self.device)\n",
    "        global_dict = global_model.state_dict()\n",
    "        for k in global_dict.keys():\n",
    "            global_dict[k] = torch.stack([local_models[i].state_dict()[k].float() for i in range(self.num_users)], 0).mean(0)\n",
    "        global_model.load_state_dict(global_dict)\n",
    "        for local_model in local_models:\n",
    "            local_model.load_state_dict(global_dict)\n",
    "        return global_model\n",
    "\n",
    "    def fed_adam(self, global_model, local_models, avg_grads):\n",
    "        global_model.train()\n",
    "        global_model.to(self.device)\n",
    "        for name, param in global_model.named_parameters():\n",
    "            param.grad = torch.stack([avg_grads[i][name] for i in range(self.num_users)], 0).mean(0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        for local_model in local_models:\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "        return global_model\n",
    "    \n",
    "    def fed_prox(self, global_model, local_models, mu=0.01):\n",
    "        global_model.train()\n",
    "        global_model.to(self.device)\n",
    "        for k in global_model.state_dict().keys():\n",
    "            global_model.state_dict()[k] = torch.stack([local_models[i].state_dict()[k].float() for i in range(self.num_users)], 0).mean(0) - mu * sum([torch.norm(local_models[i].state_dict()[k] - global_model.state_dict()[k]) for i in range(self.num_users)])\n",
    "        for local_model in local_models:\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "        return global_model\n",
    "        \n",
    "    def evaluate(self, model, val_loader):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "        model.train()\n",
    "        return running_loss / len(val_loader.dataset)                    \n",
    "            \n",
    "    def train(self, local_models, global_model, train_loaders, val_loader=None):\n",
    "        \n",
    "        global_model.to(self.device)\n",
    "        for local_model in local_models:\n",
    "            local_model.to(self.device)\n",
    "\n",
    "        if self.algorithm == 'FedAvg':\n",
    "            self.optimizer = optim.SGD(global_model.parameters(), lr=0.1)\n",
    "            for i, local_model in enumerate(local_models):\n",
    "                self.local_optimizers[i] = optim.SGD(local_model.parameters(), lr=0.1)\n",
    "        elif self.algorithm == 'FedAdam':\n",
    "            self.optimizer = optim.Adam(global_model.parameters(), lr=0.1)\n",
    "            for i, local_model in enumerate(local_models):\n",
    "                self.local_optimizers[i] = optim.Adam(local_model.parameters(), lr=0.1)\n",
    "        elif self.algorithm == 'FedProx':\n",
    "            # self.optimizer = optim.SGD(global_model.parameters(), lr=0.1)\n",
    "            for i, local_model in enumerate(local_models):\n",
    "                self.local_optimizers[i] = optim.SGD(local_model.parameters(), lr=0.1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid algorithm\")\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            avg_grad = []\n",
    "            for i, train_loader in enumerate(train_loaders):\n",
    "                local_model = local_models[i]                \n",
    "                if self.algorithm == 'FedAvg':\n",
    "                    self.train_local_model_avg(local_model, train_loader, self.local_optimizers[i], self.local_criteria[i])    \n",
    "                elif self.algorithm == 'FedAdam':\n",
    "                    avg_grad.append(self.train_local_model_adam(local_model, train_loader, self.local_optimizers[i], self.local_criteria[i]))\n",
    "                else:\n",
    "                    self.train_local_model_prox(local_model, train_loader, self.local_optimizers[i], self.local_criteria[i], global_model)\n",
    "\n",
    "            if self.algorithm == 'FedAvg':\n",
    "                global_model = self.fed_avg(global_model, local_models)\n",
    "            elif self.algorithm == 'FedAdam':\n",
    "                global_model = self.fed_adam(global_model, local_models, avg_grad)\n",
    "            else:\n",
    "                global_model = self.fed_prox(global_model, local_models)\n",
    "                                \n",
    "            if val_loader is not None:\n",
    "                val_accuracy = self.evaluate(global_model, val_loader)\n",
    "                print(f\"Epoch {epoch+1}, Validation Loss: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create heterogenous data loaders\n",
    "num_users = 25\n",
    "heterogenous_loaders = create_heterogenous_data_loaders(train_dataset, num_users)\n",
    "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: FedAvg\n",
      "Epoch 1, Validation Loss: 2.3029593204498293\n",
      "Epoch 2, Validation Loss: 2.302195877456665\n",
      "Epoch 3, Validation Loss: 2.300949782562256\n",
      "Epoch 4, Validation Loss: 2.2901844814300536\n",
      "Epoch 5, Validation Loss: 2.108607790374756\n",
      "Epoch 6, Validation Loss: 1.9677748048782349\n",
      "Epoch 7, Validation Loss: 1.8831800437927246\n",
      "Epoch 8, Validation Loss: 1.7995524208068847\n",
      "Epoch 9, Validation Loss: 1.728608934211731\n",
      "Epoch 10, Validation Loss: 1.6675622169494628\n",
      "\n",
      "Algorithm: FedAdam\n",
      "Epoch 1, Validation Loss: 1185.5081931640625\n",
      "Epoch 2, Validation Loss: 9.489222327423096\n",
      "Epoch 3, Validation Loss: 2.582181950378418\n",
      "Epoch 4, Validation Loss: 2.307996007156372\n",
      "Epoch 5, Validation Loss: 2.3119687629699706\n",
      "Epoch 6, Validation Loss: 2.315098455429077\n",
      "Epoch 7, Validation Loss: 2.3183091468811035\n",
      "Epoch 8, Validation Loss: 2.312909606552124\n",
      "Epoch 9, Validation Loss: 2.3110937377929686\n",
      "Epoch 10, Validation Loss: 2.311985194778442\n",
      "\n",
      "Algorithm: FedProx\n",
      "Epoch 1, Validation Loss: 2.3026687561035155\n",
      "Epoch 2, Validation Loss: 2.302610874557495\n",
      "Epoch 3, Validation Loss: 2.302566449356079\n",
      "Epoch 4, Validation Loss: 2.302554595184326\n",
      "Epoch 5, Validation Loss: 2.3025349136352538\n",
      "Epoch 6, Validation Loss: 2.3025250129699706\n",
      "Epoch 7, Validation Loss: 2.302514902496338\n",
      "Epoch 8, Validation Loss: 2.3025109046936034\n",
      "Epoch 9, Validation Loss: 2.3025090423583983\n",
      "Epoch 10, Validation Loss: 2.3025060134887694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['FedAvg', 'FedAdam', 'FedProx']\n",
    "for algorithm in algorithms:\n",
    "    print(f\"Algorithm: {algorithm}\")\n",
    "    global_model = CNN()\n",
    "    local_models = [CNN() for _ in range(num_users)]\n",
    "    fed_learning = FedLearning(num_users, 10, algorithm)\n",
    "    fed_learning.train(local_models, global_model, heterogenous_loaders, val_loader)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
